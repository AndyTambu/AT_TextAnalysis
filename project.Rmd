---
title: "AI an philosophy, a descriptivew text analysis overview"
author: "Andrea Tamburini"
date: "February 15, 2023"
output:
  html_document:
  df_print: paged
---
  
```{r}
library(lubridate)
library(magrittr)
library(plotly)
library(rtweet)
library(tidyverse)
library(tm)
library(tidytext)
library(wordcloud)
library(viridis)
library('lexicon')
library(MASS)
library(topicmodels)

```
  
  
  
  
  
  
  
  
  
  
  
  
  # Twitter data analysis, topic modelling and sentiment analysis
  
  ### Data
  
  For the project I consulted the PhD candidate Leonie MÃ¶ck from the department of Philosophy. She gave me some suggestions regardign some authors whose focus is mostly on philosophy of  technology and who often engage with the problematic role that AI started having in our life.

  

Lets load the data first and take a look. Each row represents one news article.

```{r}
## load rtweet
## load the tidyverse
# ## store api keys (these are fake example values; replace with your own keys)
# api_key <- "yBUKMtVObxyq0FgJQfKas0FlN"
# api_secret_key <- "f1KS00WgexCdlh8fYLIc1XJu4MXi6GLFgOdkekeQcJwUx7zMII"
# ## authenticate via web browser
# token <- create_token(
#   app = "rstatsjournalismresearch",
#   consumer_key = api_key,
#   consumer_secret = api_secret_key)
# get_token()
# 
# rt <- search_tweets("#DataScience", n = 1000, include_rts = FALSE)
# View(rt)
# 
# auth_get()
# #here is where I authenticate into my twitter app. 
# my_app <-rtweet_app(
#   "AAAAAAAAAAAAAAAAAAAAAIu%2BKQEAAAAAeqDhfFHMTUIbxX%2FWejDsB80JF0s%3DcAxcbtzPMuHf3XxGsGQKxE66pgsGA7CJs8UK0GUkNvk7NTtgfE"
# )
# 
# auth_as(my_app)
# 

# here I select the authors whose content I want to analyse
auths <- c('Floridi', 'katecrawford', 'spillteori', 'SSorgner', 'PaulNemitz', 'SvenNyholm', 'jud1ths1mon', 
'EthicsInAI', 'AdaLovelaceInst', 'TechneSPT', 'David_Gunkel', 'VincentCMueller', 'Dr_Atoosa', 'emilymbender',
'sethlazar', 'ppverbeek', 'WesselReijers', 'symboliev')


# all_the_tweets_long <- lapply(auths, function(x) get_timelines(x,n = 10000, language = 'en') %>% as_tibble())




```
This schuck of code takes relatively long to download all the tweets, this is why you can directly source them from the data I provided with the project file. 
```{r}

load('all_the_tweets_long.rds')

# in this way we can see how the 


```




These tweets come out of the function as a list. What I do in the following chuck on code is to transfrom them in a Tibble assigning also a new columsn so that they can be associated more easily with the author. 



```{r}
all_the_tweets = all_the_tweets_long


selected_columns <- lapply(seq_along(all_the_tweets), function(i) {
  
  df <- all_the_tweets[[i]]

  dplyr::mutate(dplyr::select(dplyr::filter(df, lang =='en'),created_at,full_text), author=  auths[[i]])
  
})
                       
                
tibble_tweets <- bind_rows(selected_columns)
  
  
```



Ok the first basic analysis which we have to do is to analyse the kind of time-line we are dealing with.

```{r}
tibble_tweets %<>%
  mutate(
    created = created_at %>%
      # Remove zeros.
      str_remove_all(pattern = '\\+0000') %>%
      # Parse date.
      parse_date_time(orders = '%y-%m-%d %H%M%S')
  )



tibble_tweets %<>% 
  mutate(Created_At_Round = created%>% round(units = 'hours') %>% as.POSIXct())

tibble_tweets %>% pull(created) %>% min()
## [1] "2021-10-05 01:34:17 UTC"
tibble_tweets %>% pull(created) %>% max()
## [1] "2021-10-08 01:25:52 UTC"
plt <- tibble_tweets %>% 
  dplyr::count(Created_At_Round) %>% 
  ggplot(mapping = aes(x = Created_At_Round, y = n)) +
  theme_light() +
  geom_line() +
  xlab(label = 'Date') +
  ylab(label = NULL) +
  ggtitle(label = 'Number of Tweets per Hour')

plt %>% ggplotly()
```
Well it is quite onbvious that the accounts where active at very different times. We can see if actually makes sense to have the whole timeline or maybe reduce it.

```{r}

plt <- tibble_tweets %>% 
  group_by(author) %>% 
  dplyr::count(Created_At_Round) %>% 
  ggplot(mapping = aes(x = Created_At_Round, y = n)) +
  theme_light() +
  geom_line() +
  facet_wrap(~author)+
  xlab(label = 'Date') +
  ylab(label = NULL) +
  ggtitle(label = 'Number of Tweets per Hour')

plt %>% ggplotly()
```

Let us now inspect the column `text`, the column with our text to be analyzed. 

```{r}



head(tibble_tweets$full_text) # show the first lines of the column
class(tibble_tweets$full_text) # check the class of the column.

# ok the class is the correct one 


```
Ok now what we have to do is to clean a biut these data and see what comes out of them 

```{r}
## pre-processing text:
clean.text = function(x)
{
  # convert to lower case
  x = tolower(x)
  # remove rt
  x = gsub("rt ", "", x)
  # remove at
  x = gsub("@\\w+", "", x)
  # remove punctuation
  x = gsub("[[:punct:]]", "", x)
  
  x = gsub("[^[:alnum:]#\\.]+", " ", x) 
  # remove numbers
  x = gsub("[[:digit:]]", "", x)
  # remove links http
  x = gsub("http\\w+", "", x)
  # remove tabs
  x = gsub("[ |\t]{2,}", "", x)
  # remove blank spaces at the beginning
  x = gsub("^ ", "", x)
  # remove blank spaces at the end
  x = gsub(" $", "", x)
  # some other cleaning text
  x = gsub('https://','',x)
  x = gsub('http://','',x)
  x = gsub('[^[:graph:]]', ' ',x)
  x = gsub('[[:punct:]]', '', x)
  x = gsub('[[:cntrl:]]', '', x)
  x = gsub('\\d+', '', x)
  x = str_replace_all(x,"[^[:graph:]]", " ")
  return(x)
}

tibble_tweets$full_text



cleanText <- clean.text(tibble_tweets$full_text)
# remove empty results (if any)
idx <- which(cleanText == " ")
cleanText <- cleanText[cleanText != " "]


# after checking for its quality we assign the new column usig the double pipe fro magrittr
tibble_tweets %<>%
  mutate(cleanText = clean.text(tibble_tweets$full_text))



```

The first analysis that we can do is to make a words counting 


```{r}
# Convert the text documents into a corpus
corpus <- Corpus(VectorSource(tibble_tweets$cleanText))
# Remove stop words from the corpus
corpus <- tm_map(corpus, removeWords,  c("ai","will", "well","like","new",'the','amp', 'thanks', 'thank', 'good','can','just','one','us',
                                         'gtgt', 'use', 'join', 'make', 'really', 'im', 'way',
                                         stopwords("english"), 
                                         lexicon::function_words))
# Convert the resulting corpus back into a character vector
text_docs_without_stop_words <- sapply(corpus, as.character)


tibble_tweets_clean <- tibble_tweets

tibble_tweets_clean$text <- text_docs_without_stop_words


tweets_words <- tibble_tweets_clean %>%
  unnest_tokens(word, text) %>%
  dplyr::count(word, sort = TRUE) %>% 
  dplyr::filter(str_length(word) >1)
```
from this first analysis we do not obtain particularly surprising results. The word counts are basically reflecting the specificity of the crown we selected. 
What we can do to make it a bit more interesting is tu visualise it in a bit more attractive manner. For this I use the wordcloud package and its basic visualisation

```{r}
tweets_words_best25 <-  tweets_words %>% 
                        filter(n > 500 )

n_colors <- 8
colors <- brewer.pal(n_colors, "Dark2")


wordcloud(tweets_words_best25$word, tweets_words_best25$n, scale=c(5, 0.1), colors=colors, random.order = FALSE)
```

# Sentiment analysis

In the next chuncks I work on some sentiment analysis regardign the tweets we downloaded. I am using the structure and the structure which I found online at [djdjdjf]


```{r}
positive = scan('./words/positive-words.txt', what = 'character', comment.char = ';')
negative = scan('./words/negative-words.txt', what = 'character', comment.char = ';')
# add your list of words below as you wish if missing in above read lists
pos.words = c(positive,'ethical','aware','regulated')
neg.words = c(negative,'unregulated','invasive','uncontrolled')
```

The next function is also taken fromt he same link. Anyway, I added some commenting. 

```{r}
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
  require(plyr)
  require(stringr)
  
  # we are giving vector of sentences as input. 
  # plyr will handle a list or a vector as an "l" for us
  # we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    
    # clean up sentences with R's regex-driven global substitute, gsub() function:
    sentence = gsub('https://','',sentence)
    sentence = gsub('http://','',sentence)
    sentence = gsub('[^[:graph:]]', ' ',sentence)
    sentence = gsub('[[:punct:]]', '', sentence)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    sentence = str_replace_all(sentence,"[^[:graph:]]", " ")
    # and convert to lower case:
    sentence = tolower(sentence)
    
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    
    # compare our words to the dictionaries of positive & negative terms
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    
    # match() returns the position of the matched term or NA
    # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    
    # TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentences)
  return(scores.df)
}

score_tbl <- score.sentiment(tibble_tweets$cleanText, pos.words, neg.words)
# sentiment score frequency table
table(score_tbl$score)
```




```{r}
score_tbl %>%
  ggplot(aes(x=score, fill= ..count..)) + 
  geom_histogram(binwidth = 0.5)+ 
  ylab("Frequency") + 
  xlab("sentiment score") +
  ggtitle("Distribution of Sentiment scores of the tweets") +
  theme_minimal()
  
  
```
The majority of the tweets are not directly categorised into positive or negative content. This might be due to how we select the words (we might be missing something) or to the relative careful tone which the selected authors have in expressing their opinions.


What we can do now, is again a Wordclouds visualisation but enriched with the levelling of the positive-negative categorisation. 



```{r}
# Convert the text documents into a corpus
corpus <- Corpus(VectorSource(tibble_tweets$cleanText))
# Remove stop words from the corpus
corpus <- tm_map(corpus, removeWords,   c("ai","will", "well","like","new",'the','amp', 'thanks', 'thank', 'good','can','just','one','us',
                                         'gtgt', 'use', 'join', 'make', 'really', 'im', 'way',
                                         stopwords("english"), 
                                         lexicon::function_words))
# Convert the resulting corpus back into a character vector
text_docs_without_stop_words <- sapply(corpus, as.character)


tibble_tweets_clean <- tibble_tweets

tibble_tweets_clean$text <- text_docs_without_stop_words


tweets_words <- tibble_tweets_clean %>%
  unnest_tokens(word, text) %>%
  dplyr::count(word, sort = TRUE) %>% 
  dplyr::filter(str_length(word) >1)
```


```{r}
tibble_tweets_pos <- 
  tibble_tweets %>% 
  distinct(cleanText) %>% 
  right_join(score_tbl %>% ungroup() %>%   filter(score > 0), by = c('cleanText' = 'text'))
  

corpus_pos <- Corpus(VectorSource(tibble_tweets_pos$cleanText))
# Remove stop words from the corpus
corpus_pos <- tm_map(corpus_pos, removeWords,  c("ai","will", "well","like","new",'the','amp', 'thanks', 'thank', 'good','can','just','one','us',
                                         'gtgt', 'use', 'join', 'make', 'really', 'im', 'way','great',
                                         stopwords("english"), 
                                         lexicon::function_words))
# Convert the resulting corpus back into a character vector
text_docs_without_stop_words_pos <- sapply(corpus_pos, as.character)


tibble_tweets_clean_pos <- tibble_tweets_pos

tibble_tweets_clean_pos$text <- text_docs_without_stop_words_pos


tweets_words_pos <- tibble_tweets_clean_pos %>%
  unnest_tokens(word, text) %>%
  dplyr::count(word, sort = TRUE) %>% 
  dplyr::filter(str_length(word) >1)



tweets_words_best15_pos <-  tweets_words_pos %>% 
                        filter(n > 300 )

n_colors <- 8
colors <- brewer.pal(n_colors, "Dark2")


wordcloud(tweets_words_best15_pos$word, tweets_words_best15_pos$n, scale=c(4, 0.1), colors=colors, random.order = FALSE)

```


```{r}
tibble_tweets_neg <- 
  tibble_tweets %>% 
  distinct(cleanText) %>% 
  right_join(score_tbl %>% ungroup() %>%   filter(score < 0), by = c('cleanText' = 'text'))
  

corpus_neg <- Corpus(VectorSource(tibble_tweets_neg$cleanText))
# Remove stop words from the corpus
corpus_neg <- tm_map(corpus_neg, removeWords,  c("ai","will", "well","like","new",'the','amp', 'thanks', 'thank', 'good',
                                                 'can','just','one','us', 'gtgt', 'use', 'join', 'make', 'really', 'im', 
                                                 'way','great', 'issue', 'issues', 'time', 'problem','dont', 'find', 'bad',
                                                 'isnt','see','seems','hard', 'said', 'eu', 'uk', 'don', 'lot', 'etc', 'theres', 'thats', 'news',
                                         'year', 'least',
                                         stopwords("english"), 
                                         lexicon::function_words)) 
# Convert the resulting corpus back into a character vector
text_docs_without_stop_words_neg <- sapply(corpus_neg, as.character)


tibble_tweets_clean_neg <- tibble_tweets_neg

tibble_tweets_clean_neg$text <- text_docs_without_stop_words_neg


tweets_words_neg <- tibble_tweets_clean_neg %>%
  unnest_tokens(word, text) %>%
  dplyr::count(word, sort = TRUE) %>% 
  dplyr::filter(str_length(word) >1)



tweets_words_best25_neg <-  tweets_words_neg %>% 
                        filter(n > 100 )

# tibble_tweets_clean$cleanText[grepl("chatgpt",tibble_tweets_clean$cleanText)]

n_colors <- 8
colors <- brewer.pal(n_colors, "Dark2")


wordcloud(tweets_words_best25_neg$word, tweets_words_best25_neg$n, scale=c(4, 0.1), colors=colors, random.order = FALSE)

```


Ok at this point is we can have a look at some topic modelling 


# topic modelling 


```{r}



tweets_words <- tibble_tweets_clean %>%
  unnest_tokens(word, text) %>%
  dplyr::count(word, sort = TRUE) %>% 
  dplyr::filter(str_length(word) >1)





doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))
dtm <- DocumentTermMatrix(corpus[doc.lengths > 0])
# model <- LDA(dtm, 10)  # Go ahead and test a simple model if you want
# Run LDA with different number of topics
#LDA model with 5 topics selected
lda_5 = LDA(dtm, k = 5, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 2 topics selected
lda_2 = LDA(dtm, k = 2, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
lda_10 = LDA(dtm, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))
```
















